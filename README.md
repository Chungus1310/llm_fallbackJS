# llm_fallbackJS ğŸ¤–

A robust TypeScript library for LLM API management with automatic fallback support. Keep your AI-powered applications running smoothly even when primary providers fail.

## Features âœ¨

- ğŸ”„ Automatic failover between multiple LLM providers
- ğŸ¯ Built-in support for popular LLM APIs:
  - OpenRouter (with Mistral 7B Instruct)
  - HuggingFace (featuring Mistral-Nemo-Instruct-2407)
  - NVIDIA AI (with Llama 3.3 70B)
  - Google's Gemini 1.5
- âš¡ Easy integration with existing projects
- ğŸ› ï¸ Configurable options for each provider
- ğŸ”‘ Simple API key management through environment variables

## Project Structure ğŸ“

```
your-project/              # Your main project directory
â”œâ”€â”€ llm_fallbackJS/       # This module
â”‚   â”œâ”€â”€ client.ts         # Main client implementation
â”‚   â”œâ”€â”€ providers.ts      # LLM provider implementations
â”‚   â”œâ”€â”€ exceptions.ts     # Custom error definitions
â”‚   â”œâ”€â”€ index.ts         # Public API exports
â”‚   â”œâ”€â”€ utils.ts         # Utility functions
â”‚   â”œâ”€â”€ package.json     # Dependencies and project config
â”‚   â””â”€â”€ README.md        # This documentation
â”œâ”€â”€ .env                 # Your API keys (create this)
â””â”€â”€ your-code.ts        # Your implementation
```

## Installation ğŸ“¦

```bash
# Clone the repository into your project
git clone https://github.com/Chungus1310/llm_fallbackJS.git

# Install dependencies
cd llm_fallbackJS
npm install
```

## Setup ğŸš€

1. Create a `.env` file in your project root:

```env
OPENROUTER_API_KEY=your_openrouter_key
HUGGINGFACE_API_KEY=your_huggingface_key
NVIDIA_API_KEY=your_nvidia_key
GEMINI_API_KEY=your_gemini_key
```

2. Import and use in your project:

```typescript
import { LLMClient } from './llm_fallbackJS';

// Create client with default providers (OpenRouter, HuggingFace, NVIDIA, Gemini)
const client = new LLMClient();

// Generate text with automatic fallback
try {
    const response = await client.generate("What is the meaning of life?", {
        temperature: 0.7,
        max_tokens: 1000
    });
    console.log(response);
} catch (error) {
    console.error('Generation failed:', error);
}
```

## Advanced Usage ğŸ”§

### Custom Provider Configuration

```typescript
import { 
    LLMClient, 
    OpenRouterProvider,
    HuggingFaceProvider,
    NvidiaProvider,
    GeminiProvider 
} from './llm_fallbackJS';

// Initialize OpenRouter with custom site info
const openRouter = new OpenRouterProvider(
    process.env.OPENROUTER_API_KEY,
    { url: 'https://your-site.com', name: 'Your App Name' }
);

// Initialize other providers
const huggingFace = new HuggingFaceProvider(process.env.HUGGINGFACE_API_KEY);
const nvidia = new NvidiaProvider(process.env.NVIDIA_API_KEY);
const gemini = new GeminiProvider(process.env.GEMINI_API_KEY);

// Create client with specific provider order
const client = new LLMClient([openRouter, huggingFace, nvidia, gemini]);
```

### Provider-Specific Options

```typescript
// Each provider supports these options:
const response = await client.generate("Your prompt", {
    temperature: 0.7,     // Controls randomness (0-1)
    max_tokens: 1000      // Maximum length of response
});
```

### Error Handling

```typescript
import { AllProvidersFailedError } from './llm_fallbackJS';

try {
    const response = await client.generate("Your prompt");
    console.log(response);
} catch (error) {
    if (error instanceof AllProvidersFailedError) {
        console.error("All LLM providers failed");
    } else {
        console.error("Unexpected error:", error);
    }
}
```

## Provider Details ğŸ“‹

### OpenRouter
- Default model: `mistralai/mistral-7b-instruct:free`
- Requires API key and site information
- Full OpenAI-compatible API interface

### HuggingFace
- Default model: `mistralai/Mistral-Nemo-Instruct-2407`
- Automatic response cleaning
- Configurable model selection

### NVIDIA AI
- Default model: `meta/llama-3.3-70b-instruct`
- Advanced parameter configuration
- High-performance inference

### Gemini
- Default model: `gemini-1.5-flash`
- Google's latest model
- Optimized for quick responses

## Contributing ğŸ¤

Contributions are welcome! Feel free to:

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License ğŸ“„

This project is licensed under the ISC License - see the LICENSE file for details.

## Support ğŸ’¬

For issues, questions, or contributions, please visit our [GitHub repository](https://github.com/Chungus1310/llm_fallbackJS/issues).

---

Made with â¤ï¸ by [Chun](https://github.com/Chungus1310)
